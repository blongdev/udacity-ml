{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries that will be usedin this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the twitter data into a panda dataframe. I originally ran into an encoding issue and had to save the csv in UTF-8. The csv file does not include a column header row, so add those in manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"twitter_data.csv\", header=None, names=[\"sentiment\", \"tweet_id\", \"date\", \"query\", \"user\", \"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment    tweet_id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user  \\\n",
       "0  _TheSpecialOne_   \n",
       "1    scotthamilton   \n",
       "2         mattycus   \n",
       "3          ElleCTF   \n",
       "4           Karoli   \n",
       "\n",
       "                                                                                                                 tweet  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "2                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "3                                                                      my whole body feels itchy and like its on fire   \n",
       "4      @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that dont seem useful for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0          0   \n",
       "1          0   \n",
       "2          0   \n",
       "3          0   \n",
       "4          0   \n",
       "\n",
       "                                                                                                                 tweet  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!  \n",
       "2                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds  \n",
       "3                                                                      my whole body feels itchy and like its on fire   \n",
       "4      @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([\"tweet_id\", \"date\", \"query\", \"user\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT KEEP THIS IN THE PROJECT!!!\n",
    "DROPPING A BUNCH OF ROWS TO SPEED UP PREPROCESSING WHILE TESTING!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(300000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def processTweet(tweet):\n",
    "    tweet = re.sub(\"[@|#]\\w+\\S\",\"\", tweet) # remove @usernames and #hashtags\n",
    "    tweet = re.sub(\"http[s]?://[\\S]+\", '', tweet) # remove urls \n",
    "    tweet = re.sub(r\"(.)\\1\\1+\",r\"\\1\\1\", tweet) # remove letters that repeat more than 2 times\n",
    "    tweet = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', '',tweet) # remove punctuation\n",
    "    tweet = tweet.lower() # lowercase\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1035544</th>\n",
       "      <td>4</td>\n",
       "      <td>i am watching taken with the family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362508</th>\n",
       "      <td>4</td>\n",
       "      <td>i spend most of my time caring about others because i give and give and give some moreso yes i would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710278</th>\n",
       "      <td>0</td>\n",
       "      <td>i just hit a bird i killed it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249710</th>\n",
       "      <td>0</td>\n",
       "      <td>my dog was eating my hair i thought she was being weird so i left ampnoticed syrup from my french toast in my hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506379</th>\n",
       "      <td>0</td>\n",
       "      <td>wishing jason d wasnt in actra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment  \\\n",
       "1035544          4   \n",
       "1362508          4   \n",
       "710278           0   \n",
       "249710           0   \n",
       "506379           0   \n",
       "\n",
       "                                                                                                                       tweet  \n",
       "1035544                                                                                 i am watching taken with the family   \n",
       "1362508                i spend most of my time caring about others because i give and give and give some moreso yes i would   \n",
       "710278                                                                                        i just hit a bird i killed it   \n",
       "249710   my dog was eating my hair i thought she was being weird so i left ampnoticed syrup from my french toast in my hair   \n",
       "506379                                                                                       wishing jason d wasnt in actra   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].map(lambda tweet: processTweet(tweet))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['sentiment'].map({0: 0, 4: 1})\n",
    "X = df.drop(['sentiment'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "train_vector = count_vector.fit_transform(X_train['tweet'])\n",
    "test_vector = count_vector.transform(X_test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = naive_bayes.predict(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30055  7411]\n",
      " [ 9799 27735]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.78     37466\n",
      "           1       0.79      0.74      0.76     37534\n",
      "\n",
      "    accuracy                           0.77     75000\n",
      "   macro avg       0.77      0.77      0.77     75000\n",
      "weighted avg       0.77      0.77      0.77     75000\n",
      "\n",
      "0.7705333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW HERE STARTING RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords \n",
    "# from nltk.tokenize import word_tokenize \n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# X_train['tokens'] = X_train['tweet'].apply(word_tokenize)\n",
    "\n",
    "# X_train['tokens'] = X_train['tokens'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence, text\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vocabulary_size = 5000\n",
    "\n",
    "\n",
    "# count_vector2 = TfidfVectorizer(max_features=vocabulary_size, stop_words=\"english\")\n",
    "\n",
    "# train_vector2 = count_vector2.fit_transform(X_train['tweet'])\n",
    "# test_vector2 = count_vector2.transform(X_test['tweet'])\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train['tweet'])\n",
    "\n",
    "train_token = tokenizer.texts_to_sequences(X_train['tweet'])\n",
    "test_token = tokenizer.texts_to_sequences(X_test['tweet'])\n",
    "\n",
    "max_words = 50\n",
    "train_padded = sequence.pad_sequences(train_token, maxlen=max_words)\n",
    "test_padded = sequence.pad_sequences(test_token, maxlen=max_words)\n",
    "\n",
    "# train_padded = tokenizer.texts_to_matrix(X_train['tweet'], mode=\"count\")\n",
    "# test_padded = tokenizer.texts_to_matrix(X_test['tweet'], mode=\"count\")\n",
    "\n",
    "# y_train2= to_categorical(y_train, 2)\n",
    "# y_test2 = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 256)           1280000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,805,569\n",
      "Trainable params: 1,805,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Activation\n",
    "\n",
    "embedding_size=256\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512, input_dim=vocabulary_size))\n",
    "# model.add(Activation(\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# # model.add(Dense(256))\n",
    "# # model.add(Activation(\"relu\"))          \n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(2))\n",
    "# model.add(Activation(\"softmax\"))\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'binary_crossentropy'\n",
    "# loss_func = 'categorical_crossentropy'\n",
    "model.compile(loss=loss_func, \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 45000 samples\n",
      "Epoch 1/3\n",
      "180000/180000 [==============================] - 466s 3ms/step - loss: 0.4640 - acc: 0.7790 - val_loss: 0.4352 - val_acc: 0.7965\n",
      "Epoch 2/3\n",
      "180000/180000 [==============================] - 466s 3ms/step - loss: 0.4133 - acc: 0.8088 - val_loss: 0.4264 - val_acc: 0.8000\n",
      "Epoch 3/3\n",
      "180000/180000 [==============================] - 462s 3ms/step - loss: 0.3858 - acc: 0.8247 - val_loss: 0.4318 - val_acc: 0.7981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2edde7e48>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "model.fit(train_padded, y_train, validation_split=0.2, batch_size=batch_size, epochs=num_epochs)\n",
    "# model.fit(train_vector2, y_train2, validation_split=0.2, batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7990800000063578\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_padded, y_test, verbose=0)\n",
    "# scores = model.evaluate(test_vector2, y_test2, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '100',\n",
       " '10000',\n",
       " '1030',\n",
       " '11',\n",
       " '12',\n",
       " '12th',\n",
       " '13',\n",
       " '14',\n",
       " '140',\n",
       " '15',\n",
       " '150',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '18th',\n",
       " '19',\n",
       " '1st',\n",
       " '1v100',\n",
       " '20',\n",
       " '200',\n",
       " '20000',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '28',\n",
       " '2am',\n",
       " '2d',\n",
       " '2day',\n",
       " '2gb',\n",
       " '2morrow',\n",
       " '2moz',\n",
       " '2nd',\n",
       " '2nite',\n",
       " '2pm',\n",
       " '30',\n",
       " '300th',\n",
       " '35',\n",
       " '360',\n",
       " '3am',\n",
       " '3d',\n",
       " '3g',\n",
       " '3gs',\n",
       " '3rd',\n",
       " '40',\n",
       " '400',\n",
       " '430',\n",
       " '45',\n",
       " '48',\n",
       " '4am',\n",
       " '4gb',\n",
       " '4th',\n",
       " '50',\n",
       " '500th',\n",
       " '530',\n",
       " '55',\n",
       " '5k',\n",
       " '5pm',\n",
       " '5th',\n",
       " '60',\n",
       " '630',\n",
       " '6th',\n",
       " '720',\n",
       " '73',\n",
       " '7am',\n",
       " '830',\n",
       " '85',\n",
       " '8am',\n",
       " '8pm',\n",
       " '90',\n",
       " '90210',\n",
       " '9am',\n",
       " 'aaah',\n",
       " 'aah',\n",
       " 'aahh',\n",
       " 'aaron',\n",
       " 'abandoned',\n",
       " 'abc2',\n",
       " 'abit',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'abt',\n",
       " 'abyss',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'accident',\n",
       " 'accomplished',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'aches',\n",
       " 'achievement',\n",
       " 'ack',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addicting',\n",
       " 'addiction',\n",
       " 'adidas',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adorable',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'adventure',\n",
       " 'advil',\n",
       " 'affected',\n",
       " 'afraid',\n",
       " 'afronum',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'age',\n",
       " 'agenda',\n",
       " 'ages',\n",
       " 'agghh',\n",
       " 'agh',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahaha',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ai',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'aj',\n",
       " 'aka',\n",
       " 'ako',\n",
       " 'al',\n",
       " 'alarm',\n",
       " 'alas',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'ale',\n",
       " 'alec',\n",
       " 'alex',\n",
       " 'ali',\n",
       " 'alice',\n",
       " 'alive',\n",
       " 'allergies',\n",
       " 'allison',\n",
       " 'allowed',\n",
       " 'alot',\n",
       " 'alright',\n",
       " 'amaazing',\n",
       " 'amanda',\n",
       " 'amandapalmer',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amigas',\n",
       " 'amp',\n",
       " 'ampamp',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'ana',\n",
       " 'analog',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angels',\n",
       " 'angry',\n",
       " 'anh',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'ankle',\n",
       " 'anniversary',\n",
       " 'announced',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'answer',\n",
       " 'answering',\n",
       " 'ants',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'api',\n",
       " 'apir',\n",
       " 'apologize',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appears',\n",
       " 'appi',\n",
       " 'apple',\n",
       " 'applied',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'apps',\n",
       " 'appt',\n",
       " 'apt',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'arent',\n",
       " 'argentina',\n",
       " 'argh',\n",
       " 'arguing',\n",
       " 'arm',\n",
       " 'arms',\n",
       " 'arrangement',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arriving',\n",
       " 'art',\n",
       " 'article',\n",
       " 'artist',\n",
       " 'asap',\n",
       " 'ashley',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'assignment',\n",
       " 'assume',\n",
       " 'assumption',\n",
       " 'atampt',\n",
       " 'ate',\n",
       " 'athens',\n",
       " 'atl',\n",
       " 'atlanta',\n",
       " 'atlantic',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attempt',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attended',\n",
       " 'attention',\n",
       " 'au',\n",
       " 'audio',\n",
       " 'audition',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'autograph',\n",
       " 'automatically',\n",
       " 'available',\n",
       " 'avatar',\n",
       " 'ave',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avoided',\n",
       " 'aw',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awh',\n",
       " 'awhh',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awsome',\n",
       " 'aww',\n",
       " 'aye',\n",
       " 'ayyee',\n",
       " 'azadi',\n",
       " 'b4',\n",
       " 'babe',\n",
       " 'babes',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babysitting',\n",
       " 'bac',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backyard',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'badness',\n",
       " 'bag',\n",
       " 'baguio',\n",
       " 'bah',\n",
       " 'bak',\n",
       " 'bake',\n",
       " 'baking',\n",
       " 'bald',\n",
       " 'ball',\n",
       " 'balls',\n",
       " 'bamboo',\n",
       " 'bamboozle',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bang',\n",
       " 'bangin',\n",
       " 'bank',\n",
       " 'banquet',\n",
       " 'bar',\n",
       " 'barcelona',\n",
       " 'barely',\n",
       " 'bars',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'bash',\n",
       " 'basically',\n",
       " 'bass',\n",
       " 'bastard',\n",
       " 'bat',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'battery',\n",
       " 'battle',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bbc',\n",
       " 'bbf',\n",
       " 'bbq',\n",
       " 'bbs',\n",
       " 'bc',\n",
       " 'bcz',\n",
       " 'bday',\n",
       " 'bea',\n",
       " 'beach',\n",
       " 'beans',\n",
       " 'bear',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beatles',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'bee',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'believe',\n",
       " 'bell',\n",
       " 'bella',\n",
       " 'beloved',\n",
       " 'ben',\n",
       " 'benjamin',\n",
       " 'berlin',\n",
       " 'best',\n",
       " 'bestfriend',\n",
       " 'bestie',\n",
       " 'besties',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'betta',\n",
       " 'better',\n",
       " 'beyonce',\n",
       " 'bf',\n",
       " 'bff',\n",
       " 'bgt',\n",
       " 'bib',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bike',\n",
       " 'bills',\n",
       " 'billy',\n",
       " 'bio',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birthday',\n",
       " 'biscuit',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitches',\n",
       " 'bite',\n",
       " 'biting',\n",
       " 'bits',\n",
       " 'bitter',\n",
       " 'bk',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blae',\n",
       " 'blah',\n",
       " 'blahh',\n",
       " 'blame',\n",
       " 'blast',\n",
       " 'bleebly',\n",
       " 'bleh',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blew',\n",
       " 'bling',\n",
       " 'blip',\n",
       " 'blipfm',\n",
       " 'bliss',\n",
       " 'blockbuster',\n",
       " 'blocked',\n",
       " 'blog',\n",
       " 'bloggers',\n",
       " 'blogging',\n",
       " 'blogtv',\n",
       " 'blonde',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'board',\n",
       " 'boards',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'bobby',\n",
       " 'bodies',\n",
       " 'body',\n",
       " 'bogus',\n",
       " 'bom',\n",
       " 'bomb',\n",
       " 'bonafide',\n",
       " 'bond',\n",
       " 'bones',\n",
       " 'bonfire',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'bookmark',\n",
       " 'books',\n",
       " 'boosh',\n",
       " 'boot',\n",
       " 'booze',\n",
       " 'borders',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'boss',\n",
       " 'bosses',\n",
       " 'boston',\n",
       " 'bots',\n",
       " 'bottle',\n",
       " 'bought',\n",
       " 'boulder',\n",
       " 'bound',\n",
       " 'bout',\n",
       " 'bowl',\n",
       " 'bowling',\n",
       " 'box',\n",
       " 'boxes',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boyfriends',\n",
       " 'boyle',\n",
       " 'boys',\n",
       " 'bracelet',\n",
       " 'brain',\n",
       " 'brains',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'brandon',\n",
       " 'brandy',\n",
       " 'brazil',\n",
       " 'brazilian',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breathe',\n",
       " 'breathing',\n",
       " 'brew',\n",
       " 'bride',\n",
       " 'bridge',\n",
       " 'bright',\n",
       " 'brighten',\n",
       " 'brighter',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'britain',\n",
       " 'britains',\n",
       " 'britney',\n",
       " 'bro',\n",
       " 'broadcast',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'bros',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'browser',\n",
       " 'brr',\n",
       " 'brudder',\n",
       " 'brunch',\n",
       " 'brush',\n",
       " 'bt',\n",
       " 'btw',\n",
       " 'bubble',\n",
       " 'bubbles',\n",
       " 'bucks',\n",
       " 'bucky',\n",
       " 'buddies',\n",
       " 'buddy',\n",
       " 'bug',\n",
       " 'bugs',\n",
       " 'build',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'bull',\n",
       " 'bully',\n",
       " 'bum',\n",
       " 'bumble',\n",
       " 'bummed',\n",
       " 'bummer',\n",
       " 'bunch',\n",
       " 'bunny',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burnt',\n",
       " 'bus',\n",
       " 'buses',\n",
       " 'business',\n",
       " 'busted',\n",
       " 'busters',\n",
       " 'busy',\n",
       " 'butt',\n",
       " 'butter',\n",
       " 'butterflies',\n",
       " 'button',\n",
       " 'buut',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'bye',\n",
       " 'ca',\n",
       " 'cable',\n",
       " 'cafe',\n",
       " 'cake',\n",
       " 'cakes',\n",
       " 'cal',\n",
       " 'calendar',\n",
       " 'cali',\n",
       " 'california',\n",
       " 'called',\n",
       " 'callin',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'cam',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'campus',\n",
       " 'canada',\n",
       " 'canceled',\n",
       " 'cancelled',\n",
       " 'cancer',\n",
       " 'candy',\n",
       " 'cantt',\n",
       " 'canyon',\n",
       " 'capital',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'carefully',\n",
       " 'cares',\n",
       " 'carnival',\n",
       " 'carpet',\n",
       " 'carrie',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'casting',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catholic',\n",
       " 'cats',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'cavs',\n",
       " 'cd',\n",
       " 'cds',\n",
       " 'ce',\n",
       " 'celebrate',\n",
       " 'celebration',\n",
       " 'celebrations',\n",
       " 'celebrities',\n",
       " 'celebs',\n",
       " 'cell',\n",
       " 'central',\n",
       " 'cereal',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'ch',\n",
       " 'chair',\n",
       " 'challenge',\n",
       " 'champagne',\n",
       " 'championships',\n",
       " 'champs',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'characters',\n",
       " 'charge',\n",
       " 'charleston',\n",
       " 'charm',\n",
       " 'charms',\n",
       " 'chart',\n",
       " 'charts',\n",
       " 'chase',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'cheating',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheeky',\n",
       " 'cheer',\n",
       " 'cheerful',\n",
       " 'cheering',\n",
       " 'cheerios',\n",
       " 'cheerleading',\n",
       " 'cheers',\n",
       " 'cheese',\n",
       " 'cheesecake',\n",
       " 'chef',\n",
       " 'cherry',\n",
       " 'chi',\n",
       " 'chicago',\n",
       " 'chicken',\n",
       " 'chicks',\n",
       " 'chihuahua',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'children',\n",
       " 'chile',\n",
       " 'chili',\n",
       " 'chill',\n",
       " 'chillen',\n",
       " 'chillin',\n",
       " 'chilling',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chips',\n",
       " 'chocolate',\n",
       " 'chocolates',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choked',\n",
       " 'choo',\n",
       " 'choose',\n",
       " 'chop',\n",
       " 'chore',\n",
       " 'chores',\n",
       " 'chose',\n",
       " 'chris',\n",
       " 'chriss',\n",
       " 'christmas',\n",
       " 'chuck',\n",
       " 'church',\n",
       " 'city',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classic',\n",
       " 'classroom',\n",
       " 'classy',\n",
       " 'clean',\n",
       " 'cleaned',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'clearing',\n",
       " 'clearly',\n",
       " 'cleveland',\n",
       " 'click',\n",
       " 'client',\n",
       " 'climb',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closest',\n",
       " 'closing',\n",
       " 'clothes',\n",
       " 'cloud',\n",
       " 'clouds',\n",
       " 'cloudy',\n",
       " 'club',\n",
       " 'clue',\n",
       " 'clues',\n",
       " 'coach',\n",
       " 'coast',\n",
       " 'code',\n",
       " 'coding',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'coldstone',\n",
       " 'colin',\n",
       " 'college',\n",
       " 'color',\n",
       " 'columbus',\n",
       " 'com',\n",
       " 'combination',\n",
       " 'combine',\n",
       " 'comcast',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'comfortable',\n",
       " 'comic',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'common',\n",
       " 'comp',\n",
       " 'companion',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'competition',\n",
       " 'complaint',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'computer',\n",
       " 'conan',\n",
       " 'concept',\n",
       " 'concern',\n",
       " 'concert',\n",
       " 'concerts',\n",
       " 'confirm',\n",
       " 'confirmed',\n",
       " 'confused',\n",
       " 'confusing',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'connect',\n",
       " 'connected',\n",
       " 'connection',\n",
       " 'constant',\n",
       " 'contact',\n",
       " 'contacts',\n",
       " 'contest',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'continues',\n",
       " 'control',\n",
       " 'converse',\n",
       " 'cook',\n",
       " 'cookie',\n",
       " 'cookies',\n",
       " 'cool',\n",
       " 'cooler',\n",
       " 'copies',\n",
       " 'copy',\n",
       " 'cork',\n",
       " 'corner',\n",
       " 'correct',\n",
       " 'corrupted',\n",
       " 'cos',\n",
       " 'cost',\n",
       " 'costa',\n",
       " 'costs',\n",
       " 'costumes',\n",
       " 'couch',\n",
       " 'couches',\n",
       " 'cough',\n",
       " 'coulda',\n",
       " 'couldve',\n",
       " 'count',\n",
       " 'countdown',\n",
       " 'counter',\n",
       " 'country',\n",
       " 'county',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cousin',\n",
       " 'cousins',\n",
       " 'cover',\n",
       " 'covered',\n",
       " 'covers',\n",
       " 'cow',\n",
       " 'coworkers',\n",
       " 'coz',\n",
       " 'crack',\n",
       " 'cracked',\n",
       " 'crampy',\n",
       " 'crank',\n",
       " 'cranky',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'crash',\n",
       " 'crashed',\n",
       " 'craving',\n",
       " 'crazier',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'creating',\n",
       " 'creation',\n",
       " 'creative',\n",
       " 'creature',\n",
       " 'credit',\n",
       " 'credits',\n",
       " 'crew',\n",
       " 'crib',\n",
       " 'cried',\n",
       " 'cross',\n",
       " 'crossed',\n",
       " 'crossing',\n",
       " 'crowd',\n",
       " 'crowded',\n",
       " 'cruel',\n",
       " 'cruise',\n",
       " 'crush',\n",
       " 'crying',\n",
       " 'cud',\n",
       " 'cudi',\n",
       " 'culture',\n",
       " 'cum',\n",
       " 'cup',\n",
       " 'cupcake',\n",
       " 'cupcakes',\n",
       " 'cuppa',\n",
       " 'curious',\n",
       " 'curl',\n",
       " 'curly',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'curry',\n",
       " 'custom',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cutest',\n",
       " 'cutie',\n",
       " 'cuties',\n",
       " 'cutting',\n",
       " 'cuz',\n",
       " 'cws',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'daddys',\n",
       " 'dads',\n",
       " 'daily',\n",
       " 'daisies',\n",
       " 'dallas',\n",
       " 'dam',\n",
       " 'damage',\n",
       " 'damaged',\n",
       " 'dammit',\n",
       " 'damn',\n",
       " 'damned',\n",
       " 'dance',\n",
       " 'danced',\n",
       " 'dancing',\n",
       " 'dang',\n",
       " 'dani',\n",
       " 'daniel',\n",
       " 'danny',\n",
       " 'dark',\n",
       " 'darling',\n",
       " 'darn',\n",
       " 'dat',\n",
       " 'date',\n",
       " 'dates',\n",
       " 'dating',\n",
       " 'daughter',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'davids',\n",
       " 'dawn',\n",
       " 'day',\n",
       " 'dayim',\n",
       " 'days',\n",
       " 'dc',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dealing',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'december',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'decorating',\n",
       " 'deep',\n",
       " 'deer',\n",
       " 'def',\n",
       " 'default',\n",
       " 'deff',\n",
       " 'definately',\n",
       " 'definatly',\n",
       " 'definitely',\n",
       " 'del',\n",
       " 'delayed',\n",
       " 'delays',\n",
       " 'delete',\n",
       " 'deleted',\n",
       " 'delicious',\n",
       " 'delish',\n",
       " 'delivered',\n",
       " 'delivery',\n",
       " 'dell',\n",
       " 'dem',\n",
       " 'demi',\n",
       " 'demo',\n",
       " 'demons',\n",
       " 'dentist',\n",
       " 'denver',\n",
       " 'depending',\n",
       " 'depends',\n",
       " 'depressed',\n",
       " 'depressing',\n",
       " 'derby',\n",
       " 'desert',\n",
       " 'deserve',\n",
       " 'deserved',\n",
       " 'deserves',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'designing',\n",
       " 'desk',\n",
       " 'desperate',\n",
       " 'despite',\n",
       " 'dessert',\n",
       " 'details',\n",
       " 'dev',\n",
       " 'devastated',\n",
       " 'dh',\n",
       " 'di',\n",
       " 'dia',\n",
       " 'diabetes',\n",
       " 'dick',\n",
       " 'did',\n",
       " 'diddy',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'dies',\n",
       " 'diet',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital',\n",
       " 'dinner',\n",
       " 'direct',\n",
       " 'dirty',\n",
       " 'dis',\n",
       " 'disappeared',\n",
       " 'disappointed',\n",
       " 'discount',\n",
       " 'discover',\n",
       " 'discovered',\n",
       " 'discuss',\n",
       " 'disease',\n",
       " 'dishes',\n",
       " 'disney',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
